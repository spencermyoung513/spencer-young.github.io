---
title: "A General Method for Measuring Calibration of Probabilistic Neural Regressors"
collection: publications
category: manuscripts
permalink: /publication/general
excerpt: "As machine learning systems become increasingly integrated into real-world applications, accurately representing uncertainty is crucial for enhancing their robustness and reliability. Neural networks are effective at fitting high-dimensional probability distributions but often suffer from poor calibration, leading to overconfident predictions. In the regression setting, we find that existing metrics for quantifying model calibration, such as Expected Calibration Error (ECE) and Negative Log Likelihood (NLL), introduce bias, require parametric assumptions, and suffer from information theoretic bounds on their estimating power. We propose a new approach using conditional kernel mean embeddings to measure calibration discrepancies without these shortcomings. Preliminary experiments on synthetic data demonstrate the method's potential, with future work planned for more complex applications."
date: "2025"
venue: '3rd Workshop on Uncertainty Reasoning and Quantification in Decision Making (KDD)'
paperurl: 'https://arxiv.org/abs/2405.12412v1'
bibtexurl: 'http://spencer-young.github.io/files/general.bib'
citation: 'Young, S. & Jenkins, P. (2025). &quot;A General Method for Measuring Calibration of Probabilistic Neural Regressors.&quot; <i>3rd Workshop on Uncertainty Reasoning and Quantification in Decision Making (KDD)</i>.'
---
The contents above will be part of a list of publications, if the user clicks the link for the publication than the contents of section will be rendered as a full page, allowing you to provide more information about the paper for the reader. When publications are displayed as a single page, the contents of the above "citation" field will automatically be included below this section in a smaller font.
